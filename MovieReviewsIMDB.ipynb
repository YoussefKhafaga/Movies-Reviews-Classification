{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoussefKhafaga/Movies-Reviews-Classification/blob/main/MovieReviewsIMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nltk\n",
        "! pip install transformers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "execution_count": 31,
      "metadata": {
        "id": "mvsPT0yFQdQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775a2026-1d59-4253-be60-32a5c42e8523"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644118430908
        },
        "id": "4jArGrT2MWG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "from sklearn import model_selection\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import string\n",
        "from typing_extensions import final\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644118432506
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9ps7Bw8MWHA",
        "outputId": "6ef127a1-959f-406f-f6a7-84b44ae1f6fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "htmlRGX = re.compile('<.*?>') \n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def data_preprocessing(text):\n",
        "  sentence = re.sub(htmlRGX, ' ', text)\n",
        "  sentence = \"\".join([char for char in sentence if char not in string.punctuation])\n",
        "  sentence = sentence.lower()\n",
        "  sentence = sentence.split(' ')\n",
        "  sentence = [word for word in sentence if word not in stop_words]\n",
        "  sentence = [lemmatizer.lemmatize(word=word,pos='v') for word in sentence]\n",
        "  sentence = ' '.join(sentence)\n",
        "  return sentence\n",
        "\n",
        "def split(dataFrame):\n",
        "    x, y = dataFrame.iloc[:, :-1], dataFrame.iloc[:, [-1]]  # split feature and label\n",
        "    X_train, X_rem, y_train, y_rem = train_test_split(x, y, train_size=0.7, test_size=0.3, stratify=y)\n",
        "    X_validate, X_test, y_validate, y_test = train_test_split(X_rem, y_rem, train_size=1/3, test_size=2/3,stratify=y_rem)\n",
        "    return X_train, X_validate, X_test, y_train, y_validate, y_test"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644118432604
        },
        "id": "ZRCkE4F9MWHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x,y):\n",
        "\n",
        "        self.labels =  torch.FloatTensor(y['sentiment'].values)\n",
        "        self.texts  = [tokenizer(review, \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for review in x['review']]\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return self.labels[idx]\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "        return batch_texts, batch_y"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644118432705
        },
        "id": "61NIw-qjMWHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "labels = {'negative':0,\n",
        "          'positive':1\n",
        "          }\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        self.fc2 = nn.Linear(512,256)\n",
        "        self.fc3 = nn.Linear(256,128)\n",
        "        self.fc4 = nn.Linear(128,64)\n",
        "        self.fc = nn.Linear(64,1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "         # dense layer 5 (Output layer)\n",
        "\n",
        "        #sigmoid activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "          #pass the inputs to the model  \n",
        "        _, cls_hs = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
        "        x = self.dropout(cls_hs)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "  #####\n",
        "        x1 = self.fc2(x)\n",
        "        x1 = self.bn2(x1)\n",
        "        x1 = self.relu(x1)\n",
        "\n",
        "        x1 = self.dropout(x1)\n",
        "  ######\n",
        "        x2 = self.fc3(x1)\n",
        "        x2 = self.bn3(x2)\n",
        "        x2 = self.relu(x2)\n",
        "\n",
        "        x2 = self.dropout(x2)\n",
        "  ######\n",
        "        x3 = self.fc4(x2)\n",
        "        x3 = self.bn4(x3)\n",
        "        x3 = self.relu(x3)\n",
        "\n",
        "        x3 = self.dropout(x3)\n",
        "\n",
        "        # output layer\n",
        "        x = self.fc(x3)\n",
        "        \n",
        "        # apply softmax activation\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "39u-7k262-fv",
        "gather": {
          "logged": 1644118433053
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_data):\n",
        "    test = Dataset(test_data[0],test_data[1])\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=128)\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    with torch.no_grad():\n",
        "        for test_input, test_label in test_dataloader:\n",
        "            test_label = test_label.to(device)\n",
        "            mask        = test_input['attention_mask'].to(device)\n",
        "            input_id    = test_input['input_ids'].squeeze(1).to(device)\n",
        "            output      = model(input_id, mask)\n",
        "            output      = (output >0.5).float()\n",
        "            test_label  = test_label.unsqueeze(1)\n",
        "            acc         = (output == test_label).sum().item()\n",
        "            total_acc_test += acc\n",
        "    \n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .4f}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644149251472
        },
        "id": "Ax9r0puuMWHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = []\n",
        "val_accuarcy = []\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "pTtIaYsPMWHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,path, train_dataloader, val_dataloader, learning_rate, epochs):\n",
        "    global train_accuracy,val_accuarcy,train_loss,val_loss\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "    early_stopping = EarlyStopping(path=path,patience=1, verbose=True)\n",
        "\n",
        "    if use_cuda:\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "            total_acc_train  = 0\n",
        "            total_loss_train = 0\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output     = model(input_id, mask)\n",
        "                batch_loss = criterion(output, train_label.unsqueeze(1))\n",
        "                total_loss_train += batch_loss.item()\n",
        "                output      = (output >0.5).float()\n",
        "                train_label = train_label.unsqueeze(1)\n",
        "                acc         = (output == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for val_input, val_label in val_dataloader:\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output     = model(input_id, mask)\n",
        "                    batch_loss = criterion(output, val_label.unsqueeze(1))\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    output      = (output >0.5).float()\n",
        "                    val_label = val_label.unsqueeze(1)\n",
        "                    acc         = (output == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "            \n",
        "            print(f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader.dataset): .4f} \\\n",
        "            | Train Accuracy: {total_acc_train / len(train_dataloader.dataset): .4f} | \\\n",
        "            Val Loss: {total_loss_val / len(val_dataloader.dataset): .4f} | \\\n",
        "            Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .4f}')\n",
        "\n",
        "            train_accuracy.append( total_acc_train/len(train_dataloader.dataset))\n",
        "            val_accuarcy.append( total_acc_val/len(val_dataloader.dataset) )\n",
        "            train_loss.append( total_loss_train/len(train_dataloader.dataset))\n",
        "            val_loss.append( total_loss_val/len(val_dataloader.dataset))\n",
        "\n",
        "            early_stopping(total_loss_val, model)\n",
        "        \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "                "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644148969811
        },
        "id": "h6sgDY2oMWHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "df = pd.read_csv('/content/drive/MyDrive/IMDB_Dataset.csv')\n",
        "df = df.replace({'positive':1,'negative':0})\n",
        "\n",
        "preprocess= df.copy()\n",
        "preprocess['review'] = preprocess['review'].apply(data_preprocessing)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644118488542
        },
        "id": "Y6qWaOMEMWHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644148353895
        },
        "id": "mEuN762hMWHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_validate, X_test, y_train, y_validate, y_test = split(df)\n",
        "\n",
        "train_df, val_df = Dataset(X_train,y_train), Dataset(X_validate,y_validate)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_df, batch_size=128, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_df, batch_size=128)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644118679022
        },
        "id": "H_RlHbfNMWHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "LR = [0.005,0.001,0.0005]\n",
        "\n",
        "model = None\n",
        "for lr in LR:\n",
        "    for _ in range(0,100):\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    train_accuracy = []\n",
        "    val_accuarcy = []\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    model = BertClassifier()\n",
        "    PATH = './models/model'+str(lr)\n",
        "    train(model, PATH,train_dataloader, val_dataloader, lr, EPOCHS)\n",
        "    \n",
        "    with open(f'./models/train_accuracy_{lr}','wb') as f: \n",
        "        pickle.dump( np.array(train_accuracy), f)\n",
        "\n",
        "    with open(f'./models/val_accuarcy_{lr}','wb') as f: \n",
        "        pickle.dump( np.array(val_accuarcy), f)\n",
        "\n",
        "    with open(f'./models/train_loss_{lr}','wb') as f: \n",
        "        pickle.dump( np.array(train_loss), f)\n",
        "\n",
        "    with open(f'./models/val_loss_{lr}','wb') as f: \n",
        "        pickle.dump( np.array(val_loss), f)\n",
        "    \n",
        "    epochs  =  [i for i in range(0,EPOCHS)]\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.suptitle(f'Model {lr} plot')\n",
        "    ax1.plot(epochs, val_loss,'g',epochs,train_loss,'b')\n",
        "    ax2.plot(epochs,val_accuarcy,'g',epochs,train_accuracy,'b')\n",
        "    plt.savefig('./models/model'+str(lr)+\"graph_.png\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "xhU6RZ7kMWHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = BertClassifier()\n",
        "m1 = m1.load_state_dict(torch.load(\"./models/model1\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1644149820058
        },
        "id": "37gmaEhTMWHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(m1, [X_test,y_test])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "RwPCP-W6MWHN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "MovieReviewsIMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}